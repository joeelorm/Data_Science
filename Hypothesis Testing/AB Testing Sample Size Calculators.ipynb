{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "An A/B Test is a scientific method of choosing between two options (Option A and Option B). Some examples of A/B tests include:\n",
    "\n",
    "- What number of sale items on a website makes customers most likely to purchase something: 25 or 50?\n",
    "- What color button are customers more likely to click on: blue or green?\n",
    "- Do people spend more time on a website if the background is green or orange?\n",
    "\n",
    "For A/B tests where the outcome of interest (eg., whether or not a customer makes a purchase) is categorical, an A/B test is conducted using a Chi-Square hypothesis test. In order to determine the sample size necessary for this kind of test, a sample size calculator requires three numbers:\n",
    "\n",
    "- Baseline conversion rate\n",
    "- Minimum detectable effect (also called the minimum desired lift)\n",
    "- Statistical significance threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Conversion Rate\n",
    "A/B tests usually compare an option that we’re currently using to a new option that we suspect might be better. In order to compare the two options, we need a metric. Often, our metric will be the percent of users who take a certain action after interacting with one of our options. For instance:\n",
    "\n",
    "The percent of customers who buy a t-shirt after visiting one of two versions of a website\n",
    "The percent of users who click on one of two versions of an ad\n",
    "In the t-shirt example above, the baseline conversion rate is our estimate for the percent of people who will buy a shirt under the current website design.\n",
    "\n",
    "We can generally calculate a baseline by looking at historical data for the option that we’re currently using. For example, suppose that 2000 people visited a website over the past three months and 320 of those visitors purchased a shirt. We could estimate the baseline rate as follows:\n",
    "\n",
    "$ baseline = 320/2000*100\n",
    "print(baseline) #output: 16.0 $\n",
    "\n",
    "This number may be written as a proportion (eg., 0.16) or a percent (eg., 16%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Detectable Effect\n",
    "Suppose we’re running an A/B Test to find out if a new website layout drives more subscriptions than the current one. If the new layout is only a tiny percent better, would we really care?\n",
    "\n",
    "In order to detect precise differences, we need a very large sample size. In order to choose a sample size, we need to know the smallest difference that we actually care to measure. This “smallest difference” is our desired minimum detectable effect. This is also sometimes referred to as desired lift.\n",
    "\n",
    "Minimum detectable effect or lift is generally expressed as a percent of the baseline conversion rate. Suppose that 6% of customers currently subscribe to our website (that’s our baseline conversion rate). Changing a website layout is hard, so we only think that it’s worth doing if at least 8% of our customers would subscribe with the new layout. To calculate this as a percentage of our baseline:\n",
    "\n",
    "`baseline = 6`\n",
    "\n",
    "`new = 8`\n",
    "\n",
    "`min_detectable_effect = (new - baseline) / baseline * 100`\n",
    "\n",
    "`print(min_detectable_effect) #output: 33.0`\n",
    "\n",
    "Our minimum detectable effect/desired lift is 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance Threshold\n",
    "When we run an A/B test, we usually want to use the results of the test to make a decision: use version A or B? In order to make that decision, many data scientists use a pre-determined significance threshold for their hypothesis test. For example, if we set a significance threshold of 0.05 (a commonly chosen value), we’ll “reject the null hypothesis” and conclude that the conversion rate for version B is significantly different from version A if we get a p-value less than 0.05.\n",
    "\n",
    "It turns out that this significance threshold is the false positive rate for the test: the probability of finding a significant difference when there really is none. As a business owner, we don’t want to make this kind of mistake, because then we might invest money in a change that doesn’t actually make a difference!\n",
    "\n",
    "Unfortunately, there’s a trade-off between false positives and false negatives. A false negative occurs when there is a difference between version A and B, but the test doesn’t detect it. This is a potential missed opportunity for a business owner!\n",
    "\n",
    "Most A/B test sample size calculators estimate the sample size needed for a 20% false negative rate; while a data scientist needs to choose the false positive rate they are comfortable with. The lower the false positive rate, the larger the sample size will need to be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't Interfere With Your Tests\n",
    "Suppose that a Product Manager is running an A/B Test for a redesign of a landing page. Before starting the test, she used a sample size calculator to determine the sample size: 2,200 total website visitors. After reaching 2,200 visits, she ran a Chi-Square Test. The new website design performed slightly better, but the results were not statistically significant.\n",
    "\n",
    "It might be tempting to run the test for another week to see if the difference becomes significant, but that would be a big mistake! By choosing to extend the A/B test past the original sample size, the project manager would introduce personal bias to the results of the test; she will be more likely to get the results she wants, regardless if these results reflect reality.\n",
    "\n",
    "Here are two important rules for making sure that A/B tests remain unbiased:\n",
    "\n",
    "- Don’t continue to run the test after the predetermined sample size, until “significant” results are found.\n",
    "- Don’t stop a test before reaching the predetermined sample size, just because your results reach significance early (unless there are ethical reasons that require you to stop, like a prescription drug trial).\n",
    "\n",
    "Test data is sensitive to changes in sample size, which is why it is important to calculate beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'ab.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the graph in the workspace. It shows an A/B Test where the baseline was 5%, and we want to see a lift of 50% (i.e., we want our second option to have at least a 7.5% conversion rate). A sample size calculator tells us that we need 210 observations. The chart shows the cumulative conversion rate after each new observation. When we reach our desired sample size of 210, our cumulative conversion rate is slightly higher than 5%, but the difference is not significantly different (indicated by red). By extending the experiment to 320 samples, the difference becomes significantly different (indicated by green). We might conclude that our results are significant if we stopped the experiment at this point. However, we can see this is a temporary fluctuation. After this brief moment of “significance” the conversion rate decreases and our results become insignificant again. By arbitrarily extending the study until it reaches significance, we fool ourselves!\n",
    "\n",
    "Try this: Flip a coin five times. Which side came up more frequently? Perhaps you now suspect that the coin is biased. Keep flipping the coin until that side shows up even more frequently. By changing your sample size in the middle of an experiment, you can easily convince yourself that a fair coin is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise, let’s put everything together into a single calculation. Suppose that you are running a business and want to see if a new advertisement will drive more clicks on your website. Currently, about 10% of people who see your ad are clicking on it. You want to run the new ad if at least 14% of people will click the new ad. When you run your Chi-Square test after collecting your data, you plan to use a significance threshold of 0.05, so that your chances of a false positive are relatively low. Try the following:\n",
    "\n",
    "Based on the description above, identify the baseline conversion rate and significance threshold\n",
    "Based on the description above, calculate the minimum detectable effect (hint: it’s not 4%!)\n",
    "Plug in your baseline, minimum detectable effect, and significance threshold to the provided calculator\n",
    "Calculate the total sample size needed for this experiment (note: this calculator assumes that exactly half of the sample will see each version of the ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Detectable Effect:  40.0\n",
      "significance threshold:  5\n",
      "sample size:  2060\n"
     ]
    }
   ],
   "source": [
    "MDE = ((14 - 10)/10) * 100\n",
    "print(\"Minimum Detectable Effect: \", MDE)\n",
    "\n",
    "# calculate significance threshold:\n",
    "sig_threshold = 5\n",
    "print(\"significance threshold: \", sig_threshold)\n",
    "\n",
    "# calculate total sample size: \n",
    "samp_size= 2060\n",
    "print(\"sample size: \", samp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Size Determination With Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use simulation to understand some of the considerations for setting up an A/B test: sample size, power, and the false positive rate. But before we think about designing an A/B test, let’s first remind ourselves how to conduct the test itself, after planning and collecting data.\n",
    "\n",
    "Suppose that a media company currently has a weekly newsletter email and wants to see if using the recipient’s first name in the email subject will cause more people to open the email (ie. “Bob! Checkout this week’s updates” vs “Checkout this week’s updates”). They randomly assign a group of 100 recipients to receive one of the two email subjects and record whether or not each recipient opened the email. The first few rows of their data might look something like this:\n",
    "\n",
    "\n",
    "<img src = 'image4.png'>\n",
    "\n",
    "\n",
    "In order to run a hypothesis test to decide whether there is a significant difference in the open rate for these emails, we would run a Chi-Square test. To accomplish this, we would first create a contingency table for the Email and Opened variables in the above table:\n",
    "\n",
    "`X = pd.crosstab(data.Email, data.Opened)`\n",
    "\n",
    "`print(X)`\n",
    "\n",
    "Output: <img src = 'image5.png'>\n",
    "\n",
    "\n",
    "We would then use this table to run a Chi-Square test and get a p-value:\n",
    "\n",
    "`chi2, pval, dof, expected = chi2_contingency(X)`\n",
    "\n",
    "`print(pval) #Output: 0.2186`\n",
    "\n",
    "Based on the p-value, we would make a decision about which email to use; a small p-value would provide evidence that the open rates are significantly different for the two groups, while a large p-value would suggest no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Web_Version Purchased\n",
      "0           A        no\n",
      "1           A        no\n",
      "2           A       yes\n",
      "3           A       yes\n",
      "4           A       yes\n",
      "0.10096676200907678\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "data = pd.read_csv('/Users/elorm/Documents/Repos/Datasets/ab_data.csv')\n",
    "print(data.head())\n",
    "\n",
    "# calculate contingency table here\n",
    "ab_contingency = pd.crosstab(data.Web_Version, data.Purchased)\n",
    "\n",
    "# run your chi square test here\n",
    "chi2, pval, dof, expected = chi2_contingency(ab_contingency)\n",
    "print(pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating Data for a Chi-Square test\n",
    "In the last exercise, we used some data from an A/B test to run a Chi-Square test. In the next few exercises, we’ll build up a simulation to understand the considerations that go into choosing a sample size for that test.\n",
    "\n",
    "Again consider the A/B test example from the previous exercise, comparing email subjects with and without the recipient’s first name. Suppose we know that visitors have a 50% chance of opening the control email and a 65% chance of opening the name email (30% lift!).\n",
    "\n",
    "Here we use lift to refer to the inherent difference in the distributions of our two groups of data. In the A/B Testing: Sample Size Calculators lesson, we learned that minimum detectable effect is the smallest size of the difference between the two groups that we want our test to be able to detect. If we set up our experiment with a minimum detectable effect of at least 20%, our statistical test should detect a difference with a “lift” or “effect” of 20% or greater. In this lesson we are going to simulate data that has a lift of 30% to demonstrate how the inherent lift impacts the power of our statistical test.\n",
    "\n",
    "We can use the aforementioned probabilities to simulate a dataset of 100 email recipients as follows:\n",
    "\n",
    "<img src = 'img4.png' >\n",
    "\n",
    "This gives us two simulated samples, of 50 recipients each, who hypothetically saw the name or control email subject. Each one looks something like ['yes' 'no' 'no' 'no' 'yes' 'yes' ...], where 'yes' corresponds to an opened email.\n",
    "\n",
    "Next, we can assemble these arrays into a data frame that looks a lot like the one we saw in exercise 1:\n",
    "\n",
    "<img src = 'img5.png'>\n",
    "\n",
    "<img src = 'img6.png'>\n",
    "\n",
    "Because of how we created this data frame, all of the “control” observations will be listed first, followed by all of the “name” observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Button Opened\n",
      "0  control    yes\n",
      "1  control     no\n",
      "2     name     no\n",
      "3     name    yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sample_size = 4\n",
    "lift = .3\n",
    "control_rate = .5\n",
    "name_rate = (1 + lift) * control_rate\n",
    "\n",
    "sample_control = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[control_rate,1-control_rate])\n",
    "sample_name = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[name_rate, 1-name_rate])\n",
    "\n",
    "group = ['control']*int(sample_size/2) + ['name']*int(sample_size/2)\n",
    "outcome = list(sample_control) + list(sample_name)\n",
    "sim_data = {\"Button\": group, \"Opened\": outcome}\n",
    "sim_data = pd.DataFrame(sim_data)\n",
    "print(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining Significance\n",
    "Now that we’ve practiced simulating data for an A/B test, let’s actually run a Chi-Square test for each simulated dataset and consider the decision we would make based on the outcome.\n",
    "\n",
    "If we were really running this test, we would want to use the data to make a decision about whether to use the control (old) or name (new) email subject. To make that decision, we can use a significance threshold. For example, if we’re using a significance threshold of 0.05, we’ll “reject the null hypothesis” for any p-value less than 0.05. In this context, rejecting the null would mean that we conclude that there is a significant difference between the open rates for the two email subjects and therefore we should switch to the email subject that uses the recipient’s first name.\n",
    "\n",
    "We can use the following Python statement to record whether a particular p-value is significant or not, based on a threshold of 0.05:\n",
    "\n",
    "<img src = 'img7.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Value:\n",
      "0.5385089712166174\n",
      "Result:\n",
      "not significant\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# pre-set values\n",
    "significance_threshold = 0.05\n",
    "sample_size = 100\n",
    "lift = .3\n",
    "control_rate = .5\n",
    "name_rate = (1 + lift) * control_rate\n",
    "\n",
    "# simulate a dataset\n",
    "sample_control = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[control_rate,1-control_rate])\n",
    "sample_name = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[name_rate, 1-name_rate])\n",
    "\n",
    "group = ['control']*int(sample_size/2) + ['name']*int(sample_size/2)\n",
    "outcome = list(sample_control) + list(sample_name)\n",
    "sim_data = {\"Email\": group, \"Opened\": outcome}\n",
    "sim_data = pd.DataFrame(sim_data)\n",
    "\n",
    "# run a chi-square test\n",
    "ab_contingency = pd.crosstab(np.array(sim_data.Email), np.array(sim_data.Opened))\n",
    "chi2, pval, dof, expected = chi2_contingency(ab_contingency, correction=False)\n",
    "print(\"P Value:\")\n",
    "print(pval)\n",
    "\n",
    "# determine significance here:\n",
    "result = ('significant' if pval < 0.05 else 'not significant')\n",
    "\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating Power\n",
    "In the last exercise, we learned how to simulate a dataset for a Chi-Square test, run the test, and then output a result: ‘significant’ or ‘not significant’. In this exercise, we’ll repeat that process many times so that we can inspect the relative frequency of each outcome.\n",
    "\n",
    "To do this, we’ll start by creating an empty list to store the results of our repeated experiments. Next, we’ll move all of our simulation code (to create a sample dataset, run a Chi-Square test, and determine a result) inside of a for-loop. In each iteration of the loop, we’ll append the outcome to our results list so that we can inspect it later.\n",
    "\n",
    "The outline of the code looks something like this:\n",
    "\n",
    "Set the sample size and subscription probabilities\n",
    "Create an empty list named `results`\n",
    "\n",
    "Repeat 100 times in a for-loop:\n",
    "   Simulate a dataset\n",
    "   Run a Chi-Square test\n",
    "   Use the p-value to determine significance\n",
    "   Append the result ('significant' or 'not significant') to `results`\n",
    "   \n",
    "\n",
    "Finally, we can inspect results by calculating the proportion of simulated tests where the result was 'significant':\n",
    "\n",
    "`results =  np.array(results)`\n",
    "\n",
    "`print(np.sum(results == 'significant')/100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
