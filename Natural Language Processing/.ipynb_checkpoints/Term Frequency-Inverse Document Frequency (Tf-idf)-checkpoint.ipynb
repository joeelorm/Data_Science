{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Tf-idf?\n",
    "\n",
    "Term frequency-inverse document frequency is a numerical statistic used to indicate how important a word is to each document in a collection of documents, or a corpus.\n",
    "\n",
    "When applying tf-idf to a corpus, each word is given a tf-idf score for each document, representing the relevance of that word to the particular document. A higher tf-idf score indicates a term is more important to the corresponding document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          this be a sample sentence  this be my second sentence  \\\n",
      "be                         1.000000                    1.000000   \n",
      "my                         0.000000                    1.287682   \n",
      "sample                     1.693147                    0.000000   \n",
      "second                     0.000000                    1.693147   \n",
      "sentence                   1.000000                    1.000000   \n",
      "third                      0.000000                    0.000000   \n",
      "this                       1.000000                    1.000000   \n",
      "\n",
      "          be this my third sentence  \n",
      "be                         1.000000  \n",
      "my                         1.287682  \n",
      "sample                     0.000000  \n",
      "second                     0.000000  \n",
      "sentence                   1.000000  \n",
      "third                      1.693147  \n",
      "this                       1.000000  \n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocess_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sample documents\n",
    "document_1 = \"This is a sample sentence!\"\n",
    "document_2 = \"This is my second sentence.\"\n",
    "document_3 = \"Is this my third sentence?\"\n",
    "\n",
    "# corpus of documents\n",
    "corpus = [document_1, document_2, document_3]\n",
    "\n",
    "# preprocess documents\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "corpus_index = [n for n in processed_corpus]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         data science be great  \\\n",
      "be                    1.287682   \n",
      "can                   0.000000   \n",
      "data                  1.287682   \n",
      "find                  0.000000   \n",
      "from                  0.000000   \n",
      "great                 1.287682   \n",
      "insight               0.000000   \n",
      "learn                 0.000000   \n",
      "machine               0.000000   \n",
      "science               1.287682   \n",
      "skill                 0.000000   \n",
      "the                   0.000000   \n",
      "too                   0.000000   \n",
      "with                  0.000000   \n",
      "\n",
      "         i can find insight with the skill from data science  \\\n",
      "be                                                0.000000     \n",
      "can                                               1.693147     \n",
      "data                                              1.287682     \n",
      "find                                              1.693147     \n",
      "from                                              1.693147     \n",
      "great                                             0.000000     \n",
      "insight                                           1.693147     \n",
      "learn                                             0.000000     \n",
      "machine                                           0.000000     \n",
      "science                                           1.287682     \n",
      "skill                                             1.693147     \n",
      "the                                               1.693147     \n",
      "too                                               0.000000     \n",
      "with                                              1.693147     \n",
      "\n",
      "         machine learn be great too  \n",
      "be                         1.287682  \n",
      "can                        0.000000  \n",
      "data                       0.000000  \n",
      "find                       0.000000  \n",
      "from                       0.000000  \n",
      "great                      1.287682  \n",
      "insight                    0.000000  \n",
      "learn                      1.693147  \n",
      "machine                    1.693147  \n",
      "science                    0.000000  \n",
      "skill                      0.000000  \n",
      "the                        0.000000  \n",
      "too                        1.693147  \n",
      "with                       0.000000  \n"
     ]
    }
   ],
   "source": [
    "# sample documents\n",
    "document_1 = \"Data Science is great!\"\n",
    "document_2 = \"I can find insights with the skills from Data Science.\"\n",
    "document_3 = \"Machine Learning is great too?\"\n",
    "\n",
    "# corpus of documents\n",
    "corpus = [document_1, document_2, document_3]\n",
    "\n",
    "# preprocess documents\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "corpus_index = [n for n in processed_corpus]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Term Frequency\n",
      "agonize                  1\n",
      "all                      1\n",
      "and                      1\n",
      "be                       1\n",
      "break                    1\n",
      "by                       1\n",
      "can                      1\n",
      "clear                    2\n",
      "comprehend               1\n",
      "count                    1\n",
      "day                      1\n",
      "defeat                   1\n",
      "definition               1\n",
      "die                      1\n",
      "distant                  1\n",
      "ear                      1\n",
      "er                       1\n",
      "flag                     1\n",
      "forbid                   1\n",
      "he                       1\n",
      "host                     1\n",
      "ne                       1\n",
      "nectar                   1\n",
      "need                     1\n",
      "not                      1\n",
      "of                       3\n",
      "on                       1\n",
      "one                      1\n",
      "purple                   1\n",
      "require                  1\n",
      "so                       1\n",
      "sorest                   1\n",
      "strain                   1\n",
      "succeed                  1\n",
      "success                  1\n",
      "sweet                    1\n",
      "take                     1\n",
      "tell                     1\n",
      "the                      4\n",
      "those                    1\n",
      "to                       2\n",
      "triumph                  1\n",
      "victory                  1\n",
      "who                      2\n",
      "whose                    1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "poem = '''\n",
    "Success is counted sweetest\n",
    "By those who ne'er succeed.\n",
    "To comprehend a nectar\n",
    "Requires sorest need.\n",
    "\n",
    "Not one of all the purple host\n",
    "Who took the flag to-day\n",
    "Can tell the definition,\n",
    "So clear, of victory,\n",
    "\n",
    "As he, defeated, dying,\n",
    "On whose forbidden ear\n",
    "The distant strains of triumph\n",
    "Break, agonized and clear!'''\n",
    "\n",
    "# define clear_count:\n",
    "clear_count = 2\n",
    "\n",
    "# preprocess text\n",
    "processed_poem = preprocess_text(poem)\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform([processed_poem])\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "try:\n",
    "  df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=['Term Frequency'])\n",
    "  print(df_term_frequencies)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Poem 1  Poem 2  Poem 3  Poem 4  Poem 5  Poem 6\n",
      "abash         0       0       0       0       1       0\n",
      "across        0       0       0       1       0       0\n",
      "admire        0       0       1       0       0       0\n",
      "again         0       0       0       1       0       0\n",
      "agonize       1       0       0       0       0       0\n",
      "...         ...     ...     ...     ...     ...     ...\n",
      "word          0       0       0       0       1       0\n",
      "wreck         0       0       0       1       0       0\n",
      "yet           0       0       0       0       1       0\n",
      "you           0       0       3       0       0       0\n",
      "your          0       0       1       0       0       0\n",
      "\n",
      "[173 rows x 6 columns]\n",
      "         Inverse Document Frequency\n",
      "abash                      2.252763\n",
      "across                     2.252763\n",
      "admire                     2.252763\n",
      "again                      2.252763\n",
      "agonize                    2.252763\n",
      "...                             ...\n",
      "word                       2.252763\n",
      "wreck                      2.252763\n",
      "yet                        2.252763\n",
      "you                        2.252763\n",
      "your                       2.252763\n",
      "\n",
      "[173 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
    "\n",
    "# display term-document matrix of term frequencies\n",
    "print(df_term_frequencies)\n",
    "\n",
    "# initialize and fit TfidfTransformer\n",
    "transformer = TfidfTransformer(norm = None)\n",
    "\n",
    "transformer.fit(term_frequencies)\n",
    "idf_values = transformer.idf_\n",
    "\n",
    "\n",
    "# create pandas DataFrame with inverse document frequencies\n",
    "try:\n",
    "  df_idf = pd.DataFrame(idf_values, index = feature_names, columns=['Inverse Document Frequency'])\n",
    "  print(df_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Poem 1  Poem 2    Poem 3    Poem 4    Poem 5  Poem 6\n",
      "abash    0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "across   0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "admire   0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "again    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "agonize  2.252763     0.0  0.000000  0.000000  0.000000     0.0\n",
      "...           ...     ...       ...       ...       ...     ...\n",
      "word     0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "wreck    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "yet      0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "you      0.000000     0.0  6.758289  0.000000  0.000000     0.0\n",
      "your     0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "\n",
      "[173 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from poems import poems\n",
    "# preprocess documents\n",
    "processed_poems = [preprocess_text(poem) for poem in poems]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm = None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_poems)\n",
    "\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Bag-Of-Words to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Poem 1  Poem 2  Poem 3  Poem 4  Poem 5  Poem 6\n",
      "abash         0       0       0       0       1       0\n",
      "across        0       0       0       1       0       0\n",
      "admire        0       0       1       0       0       0\n",
      "again         0       0       0       1       0       0\n",
      "agonize       1       0       0       0       0       0\n",
      "...         ...     ...     ...     ...     ...     ...\n",
      "word          0       0       0       0       1       0\n",
      "wreck         0       0       0       1       0       0\n",
      "yet           0       0       0       0       1       0\n",
      "you           0       0       3       0       0       0\n",
      "your          0       0       1       0       0       0\n",
      "\n",
      "[173 rows x 6 columns]\n",
      "           Poem 1  Poem 2    Poem 3    Poem 4    Poem 5  Poem 6\n",
      "abash    0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "across   0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "admire   0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "again    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "agonize  2.252763     0.0  0.000000  0.000000  0.000000     0.0\n",
      "...           ...     ...       ...       ...       ...     ...\n",
      "word     0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "wreck    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "yet      0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "you      0.000000     0.0  6.758289  0.000000  0.000000     0.0\n",
      "your     0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "\n",
      "[173 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from termfrequency import bow_matrix, feature_names, df_bag_of_words, corpus_index\n",
    "\n",
    "# display term-document matrix of term frequencies (bag-of-words)\n",
    "print(df_bag_of_words)\n",
    "\n",
    "# initialize and fit TfidfTransformer, transform bag-of-words matrix\n",
    "transformer = TfidfTransformer(norm = None)\n",
    "tfidf_scores = transformer.fit_transform(bow_matrix)\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index = feature_names, columns=corpus_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Once upon a midnight dreary, while I pondered, weak and weary,\n",
      " Over many a quaint and curious volume of forgotten lore,\n",
      " While I nodded, nearly napping, suddenly there came a tapping,\n",
      " As of some one gently rapping, rapping at my chamber door\n",
      "        Stanza 1  Stanza 2  Stanza 3  Stanza 4  Stanza 5   Stanza 6  Stanza 7  \\\n",
      "above        0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "adore        0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "again        0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "agree        0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "ah           0.0       0.0  3.079442       0.0       0.0   0.000000       0.0   \n",
      "...          ...       ...       ...       ...       ...        ...       ...   \n",
      "wretch       0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "yet          0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "yore         0.0       0.0  0.000000       0.0       0.0   0.000000       0.0   \n",
      "you          0.0       0.0  0.000000       0.0       0.0  10.454720       0.0   \n",
      "your         0.0       0.0  0.000000       0.0       0.0   3.484907       0.0   \n",
      "\n",
      "        Stanza 8  Stanza 9  Stanza 10  ...  Stanza 14  Stanza 15  Stanza 16  \\\n",
      "above   0.000000  4.772589        0.0  ...        0.0        0.0   0.000000   \n",
      "adore   0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "again   3.484907  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "agree   0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "ah      0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "...          ...       ...        ...  ...        ...        ...        ...   \n",
      "wretch  0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "yet     0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "yore    0.000000  3.079442        0.0  ...        0.0        0.0   6.158883   \n",
      "you     0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "your    0.000000  0.000000        0.0  ...        0.0        0.0   0.000000   \n",
      "\n",
      "        Stanza 17  Stanza 18  Stanza 19  Stanza 20  Stanza 21  Stanza 22  \\\n",
      "above    0.000000   0.000000   0.000000   2.386294        0.0   2.386294   \n",
      "adore    0.000000   0.000000   0.000000   3.484907        0.0   0.000000   \n",
      "again    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "agree    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "ah       3.079442   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "wretch   0.000000   3.484907   0.000000   0.000000        0.0   0.000000   \n",
      "yet      0.000000   0.000000   3.079442   0.000000        0.0   0.000000   \n",
      "yore     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "you      0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "your     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Stanza 23  \n",
      "above    2.386294  \n",
      "adore    0.000000  \n",
      "again    0.000000  \n",
      "agree    0.000000  \n",
      "ah       0.000000  \n",
      "...           ...  \n",
      "wretch   0.000000  \n",
      "yet      0.000000  \n",
      "yore     0.000000  \n",
      "you      0.000000  \n",
      "your     0.000000  \n",
      "\n",
      "[413 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from raven import the_raven_stanzas\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "# view first stanza\n",
    "print(the_raven_stanzas[0])\n",
    "\n",
    "\n",
    "# preprocess documents\n",
    "processed_stanzas = [preprocess_text(stanza) for stanza in the_raven_stanzas]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm = None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_stanzas)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "# get vocabulary of terms\n",
    "\n",
    "\n",
    "# get stanza index\n",
    "stanza_index = [f\"Stanza {i+1}\" for i in range(len(the_raven_stanzas))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=stanza_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Project\n",
    "\n",
    "\n",
    "##### Read the News Analysis\n",
    "Newspapers and their online formats supply the public with the information we need to understand the events occurring in the world around us. From politics to sports, the news keeps us informed, in the loop, and ready to make decisions about how to act in a rapidly changing world.\n",
    "\n",
    "Given the vast amount of news articles in circulation, identifying and organizing articles by topic is a useful activity. This can help you sift through the enormous amount of information out there so you can find the news relevant to your interests, or even allow you to build a news recommendation engine!\n",
    "\n",
    "The News International is the largest English language newspaper in Pakistan, covering local and international news across a variety of sectors. A selection of articles from a Kaggle Dataset of The News International articles is provided in the workspace.\n",
    "\n",
    "In this project you will use term frequency-inverse document frequency (tf-idf) to analyze each article’s content and uncover the terms that best describe each article, providing quick insight into each article’s topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from articles import articles\n",
    "from preprocessing import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum product prices by the federal government, Geo News reported.Sources said reduction in fares will be applicable on public transport, rickshaw, taxi and other means of traveling. Meanwhile, Karachi Transport Ittehad (KTI) has refused to abide by the government decision.KTI President Irshad Bukhari said the commuters are charged the lowest fares in Karachi as compare to other parts of the country, adding that 80pc vehicles run on Compressed Natural Gas (CNG). Bukhari said Karachi transporters will cut fares when decrease in CNG prices will be made.\n",
      "['karachi the sindh government have decide to bring down public transport fare by 7 per cent due to massive reduction in petroleum product price by the federal government geo news report source say reduction in fare will be applicable on public transport rickshaw taxi and other mean of travel meanwhile karachi transport ittehad kti have refuse to abide by the government decision kti president irshad bukhari say the commuter be charge the low fare in karachi a compare to other part of the country add that 80pc vehicle run on compress natural gas cng bukhari say karachi transporter will cut fare when decrease in cng price will be make', 'hong kong hong kong share open 0 66 percent lower monday follow a tepid lead from wall street a the first full week of the new year kick off the benchmark hang seng index dip 158 63 point to 23 699 19', 'karachi wholesale market rat for sugar drop to less than r 50 per kg follow the resumption of sugar cane crush by sugar mill in sindh within two day the rate drop by r 1 70 to r 49 80 per kg in karachi whole sale market accord to dealer the resumption of sugar cane crush by the mill stabilise the supply to the market with an immediate effect on price a well industry expert say that the quality of sugar cane be excellent in sindh and approximately 100 kg of sugar cane can produce 11 kg of sugar', 'islamabad long queue of vehicle on fuel station be visible in different part of the country a the petrol become rare commodity on thursday federal minister for petroleum shahid khaqan abbasi say it may take up to ten day to bring the situation to normality he claim that northern area of pakistan have be face the petrol shortage the minister cite the recent decline in petroleum price and delay in a shipment a reason for the shortage he say situation would improve a soon a shipment reach pakistan source tell geo news hat due to financial restraint the pakistan state oil have be unable import petrol', 'karachi the final shipment of chinese manufacture rail engine arrive in pakistan on friday federal railway minister khwaja saad rafique say the inclusion of the new engine will help ease the shortfall face by pakistan railway the shipment include 2000 and 3000 horse power engine which will be use to pull freight bogey rafique tell journalist the inclusion of 15 new engine have bring pakistan railway total strength to 268 engine however more engine be still require', 'sydney cricket fever have grip australia with the world cup just day away fan from around the world have throng to the country and hotel be capitalise price of room have almost double to 300 dollar and hotel be experience full book expert estimate that during the mega event australia will generate 1 5 million u dollar just from hotel book if the cost of internal air travel taxi and ticket be take into consideration australia stand to generate two million u dollar during the world cup', 'san francisco apple inc aim to begin produce electric vehicle a early a 2020 bloomberg report the report cite people with knowledge of the matter a say a seemingly aggressive target for a mobile device maker with little experience in car manufacture the iphone maker be push it car team of about 200 people to meet that goal but apple may decide to scrap it car make effort or delay it if executive grow unhappy with it progress the news agency say', 'lahore federal minister for railway khawaja saad rafique tuesday announce good news of pay raise for the employee of pakistan railway in a medium statement the minister disclose that a summary for increase in salary for the employee of pakistan railway have be forward to the prime minister he also say that the government have also chalk out a plan to build house for the railway worker khawaja saad rafique say it be expect that the salary of railway police may witness a jump of 20 percent he also announce the government s plan to launch a new train service between karachi and islamabad', 'islamabad the federal cabinet on tuesday approve the budget strategy paper source reveal to geo news during the cabinet meet prime minister nawaz sharif say tax rate have to be reduce to increase revenue he add that people would happily pay tax if the rate be reduce the prime minister direct the cabinet to provide maximum relief to people in the budget emphasise that the economic impact should reach people', 'beijing china will keep the yuan basically stable against a basket of currency and there be no basis for continue yuan depreciation central bank vice governor yi gang say on sunday china also will keep foreign exchange reserve at appropriate level yi say']\n",
      "  Are the tf-idf scores the same?\n",
      "0                             YES\n",
      "       Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "100            0          0          1          0          0          0   \n",
      "11             0          0          1          0          0          0   \n",
      "15             0          0          0          0          1          0   \n",
      "158            0          1          0          0          0          0   \n",
      "19             0          1          0          0          0          0   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "world          0          0          0          0          0          3   \n",
      "would          0          0          0          1          0          0   \n",
      "year           0          1          0          0          0          0   \n",
      "yi             0          0          0          0          0          0   \n",
      "yuan           0          0          0          0          0          0   \n",
      "\n",
      "       Article 7  Article 8  Article 9  Article 10  \n",
      "100            0          0          0           0  \n",
      "11             0          0          0           0  \n",
      "15             0          0          0           0  \n",
      "158            0          0          0           0  \n",
      "19             0          0          0           0  \n",
      "...          ...        ...        ...         ...  \n",
      "world          0          0          0           0  \n",
      "would          0          0          1           0  \n",
      "year           0          0          0           0  \n",
      "yi             0          0          0           2  \n",
      "yuan           0          0          0           2  \n",
      "\n",
      "[374 rows x 10 columns]\n",
      "       Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "100          0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "11           0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "15           0.0   0.000000   0.000000   0.000000   2.704748   0.000000   \n",
      "158          0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "19           0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "world        0.0   0.000000   0.000000   0.000000   0.000000   8.114244   \n",
      "would        0.0   0.000000   0.000000   2.299283   0.000000   0.000000   \n",
      "year         0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "yi           0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "yuan         0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "       Article 7  Article 8  Article 9  Article 10  \n",
      "100          0.0        0.0   0.000000    0.000000  \n",
      "11           0.0        0.0   0.000000    0.000000  \n",
      "15           0.0        0.0   0.000000    0.000000  \n",
      "158          0.0        0.0   0.000000    0.000000  \n",
      "19           0.0        0.0   0.000000    0.000000  \n",
      "...          ...        ...        ...         ...  \n",
      "world        0.0        0.0   0.000000    0.000000  \n",
      "would        0.0        0.0   2.299283    0.000000  \n",
      "year         0.0        0.0   0.000000    0.000000  \n",
      "yi           0.0        0.0   0.000000    5.409496  \n",
      "yuan         0.0        0.0   0.000000    5.409496  \n",
      "\n",
      "[374 rows x 10 columns]\n",
      "       Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "100          0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "11           0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "15           0.0   0.000000   0.000000   0.000000   2.704748   0.000000   \n",
      "158          0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "19           0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "world        0.0   0.000000   0.000000   0.000000   0.000000   8.114244   \n",
      "would        0.0   0.000000   0.000000   2.299283   0.000000   0.000000   \n",
      "year         0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "yi           0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "yuan         0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "       Article 7  Article 8  Article 9  Article 10  \n",
      "100          0.0        0.0   0.000000    0.000000  \n",
      "11           0.0        0.0   0.000000    0.000000  \n",
      "15           0.0        0.0   0.000000    0.000000  \n",
      "158          0.0        0.0   0.000000    0.000000  \n",
      "19           0.0        0.0   0.000000    0.000000  \n",
      "...          ...        ...        ...         ...  \n",
      "world        0.0        0.0   0.000000    0.000000  \n",
      "would        0.0        0.0   2.299283    0.000000  \n",
      "year         0.0        0.0   0.000000    0.000000  \n",
      "yi           0.0        0.0   0.000000    5.409496  \n",
      "yuan         0.0        0.0   0.000000    5.409496  \n",
      "\n",
      "[374 rows x 10 columns]\n",
      "[Article 1    fare\n",
      "dtype: object, Article 2    hong\n",
      "dtype: object, Article 3    sugar\n",
      "dtype: object, Article 4    petrol\n",
      "dtype: object, Article 5    engine\n",
      "dtype: object, Article 6    australia\n",
      "dtype: object, Article 7    car\n",
      "dtype: object, Article 8    railway\n",
      "dtype: object, Article 9    cabinet\n",
      "dtype: object, Article 10    china\n",
      "dtype: object]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elorm/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass input=['karachi the sindh government have decide to bring down public transport fare by 7 per cent due to massive reduction in petroleum product price by the federal government geo news report source say reduction in fare will be applicable on public transport rickshaw taxi and other mean of travel meanwhile karachi transport ittehad kti have refuse to abide by the government decision kti president irshad bukhari say the commuter be charge the low fare in karachi a compare to other part of the country add that 80pc vehicle run on compress natural gas cng bukhari say karachi transporter will cut fare when decrease in cng price will be make', 'hong kong hong kong share open 0 66 percent lower monday follow a tepid lead from wall street a the first full week of the new year kick off the benchmark hang seng index dip 158 63 point to 23 699 19', 'karachi wholesale market rat for sugar drop to less than r 50 per kg follow the resumption of sugar cane crush by sugar mill in sindh within two day the rate drop by r 1 70 to r 49 80 per kg in karachi whole sale market accord to dealer the resumption of sugar cane crush by the mill stabilise the supply to the market with an immediate effect on price a well industry expert say that the quality of sugar cane be excellent in sindh and approximately 100 kg of sugar cane can produce 11 kg of sugar', 'islamabad long queue of vehicle on fuel station be visible in different part of the country a the petrol become rare commodity on thursday federal minister for petroleum shahid khaqan abbasi say it may take up to ten day to bring the situation to normality he claim that northern area of pakistan have be face the petrol shortage the minister cite the recent decline in petroleum price and delay in a shipment a reason for the shortage he say situation would improve a soon a shipment reach pakistan source tell geo news hat due to financial restraint the pakistan state oil have be unable import petrol', 'karachi the final shipment of chinese manufacture rail engine arrive in pakistan on friday federal railway minister khwaja saad rafique say the inclusion of the new engine will help ease the shortfall face by pakistan railway the shipment include 2000 and 3000 horse power engine which will be use to pull freight bogey rafique tell journalist the inclusion of 15 new engine have bring pakistan railway total strength to 268 engine however more engine be still require', 'sydney cricket fever have grip australia with the world cup just day away fan from around the world have throng to the country and hotel be capitalise price of room have almost double to 300 dollar and hotel be experience full book expert estimate that during the mega event australia will generate 1 5 million u dollar just from hotel book if the cost of internal air travel taxi and ticket be take into consideration australia stand to generate two million u dollar during the world cup', 'san francisco apple inc aim to begin produce electric vehicle a early a 2020 bloomberg report the report cite people with knowledge of the matter a say a seemingly aggressive target for a mobile device maker with little experience in car manufacture the iphone maker be push it car team of about 200 people to meet that goal but apple may decide to scrap it car make effort or delay it if executive grow unhappy with it progress the news agency say', 'lahore federal minister for railway khawaja saad rafique tuesday announce good news of pay raise for the employee of pakistan railway in a medium statement the minister disclose that a summary for increase in salary for the employee of pakistan railway have be forward to the prime minister he also say that the government have also chalk out a plan to build house for the railway worker khawaja saad rafique say it be expect that the salary of railway police may witness a jump of 20 percent he also announce the government s plan to launch a new train service between karachi and islamabad', 'islamabad the federal cabinet on tuesday approve the budget strategy paper source reveal to geo news during the cabinet meet prime minister nawaz sharif say tax rate have to be reduce to increase revenue he add that people would happily pay tax if the rate be reduce the prime minister direct the cabinet to provide maximum relief to people in the budget emphasise that the economic impact should reach people', 'beijing china will keep the yuan basically stable against a basket of currency and there be no basis for continue yuan depreciation central bank vice governor yi gang say on sunday china also will keep foreign exchange reserve at appropriate level yi say'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# view article\n",
    "print(articles[0])\n",
    "\n",
    "# preprocess articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "print(processed_articles)\n",
    "\n",
    "\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer(processed_articles)\n",
    "\n",
    "\n",
    "# convert counts to tf-idf\n",
    "counts = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "transformer = TfidfTransformer(norm = None)\n",
    "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
    "\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm = None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# check if tf-idf scores are equal\n",
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['YES']}))\n",
    "else:\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get vocabulary of terms\n",
    "try:\n",
    "  feature_names = vectorizer.get_feature_names()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get article index\n",
    "try:\n",
    "  article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame with word counts\n",
    "try:\n",
    "  df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_word_counts)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame(s) with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get highest scoring tf-idf term for each article\n",
    "terms = [(df_tf_idf[[f'Article {i+1}']].idxmax()) for i in range(len(articles))]\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
