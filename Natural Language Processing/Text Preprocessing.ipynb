{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini\n",
      "fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "\n",
    "\n",
    "headline_no_tag = re.sub(r'<.?h1>', '', headline_one)\n",
    "\n",
    "tweet_no_at = re.sub(r'@', '', tweet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  print(headline_no_tag)\n",
    "except:\n",
    "  print('No variable called `headline_no_tag`')\n",
    "try:\n",
    "  print(tweet_no_at)\n",
    "except:\n",
    "  print('No variable called `tweet_no_at`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n",
      "Sentence Tokenization:\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  print('Word Tokenization:')\n",
    "  print(tokenized_by_word)\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_by_word`')\n",
    "try:\n",
    "  print('Sentence Tokenization:')\n",
    "  print(tokenized_by_sentence)\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_by_sentence`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization \n",
    "##### Upper or lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased brands: salvation army, ymca, boys & girls club of america\n",
      "Uppercased brands: SALVATION ARMY, YMCA, BOYS & GIRLS CLUB OF AMERICA\n"
     ]
    }
   ],
   "source": [
    "brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n",
    "\n",
    "brands_lower = brands.lower()\n",
    "\n",
    "brands_upper = brands.upper()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  print(f'Lowercased brands: {brands_lower}')\n",
    "except:\n",
    "  print('Expected a variable called `brands_lower`')\n",
    "try:\n",
    "  print(f'Uppercased brands: {brands_upper}')\n",
    "except:\n",
    "  print('Expected a variable called `brands_upper`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "print(text_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A stemmer exists:\n",
      "<PorterStemmer>\n",
      "Words Tokenized:\n",
      "['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Stemmed Words:\n",
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'It', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  print('A stemmer exists:')\n",
    "  print(stemmer)\n",
    "except:\n",
    "  print('Expected a variable called `stemmer`')\n",
    "try:\n",
    "  print('Words Tokenized:')\n",
    "  print(island_tokenized)\n",
    "except:\n",
    "  print('Expected a variable called `island_tokenized`')\n",
    "try:\n",
    "  print('Stemmed Words:')\n",
    "  print(stemmed)\n",
    "except:\n",
    "  print('Expected a variable called `stemmed`')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lemmatizer exists: <WordNetLemmatizer>\n",
      "Words Tokenized: ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Lemmatized Words: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  print(f'A lemmatizer exists: {lemmatizer}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatizer`')\n",
    "try:\n",
    "  print(f'Words Tokenized: {tokenized_string}')\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_string`')\n",
    "try:\n",
    "  print(f'Lemmatized Words: {lemmatized_words}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatized_words`')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words are: ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]\n",
    "\n",
    "\n",
    "try:\n",
    "  print(f'The lemmatized words are: {lemmatized_pos}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatized_pos`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from part_of_speech import get_part_of_speech\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "oprah_wiki = '<p>Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville\\'s WLAC-TV. </p>'\n",
    "\n",
    "oprah_edited_text = re.sub(r'</?p>', '', oprah_wiki)\n",
    "text_no_periods = re.sub(r'\\.', '', oprah_edited_text).lower()\n",
    "\n",
    "tokenized_string = word_tokenize(text_no_periods)\n",
    "\n",
    "lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
