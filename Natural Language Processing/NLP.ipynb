{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "from part_of_speech import get_part_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day,and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'escape' from 'cgi' (/Users/elorm/anaconda3/lib/python3.8/cgi.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b67151ef770a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_squids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mto_nltk_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_parsed_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36mpretty_print\u001b[0;34m(self, sentence, highlight, stream, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreeprettyprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreePrettyPrinter\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreeprettyprinter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreePrettyPrinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),\n\u001b[1;32m    700\u001b[0m               file=stream)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/treeprettyprinter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcgi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'escape' from 'cgi' (/Users/elorm/anaconda3/lib/python3.8/cgi.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "#from squids import squids_text\n",
    "\n",
    "squids_text = 'So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist\\'s bag. She hardly even noticed.'\n",
    "\n",
    "dependency_parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains.\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    " to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'egg', 'get', 'large', 'large', 'human', 'come', 'within', 'yard', 'saw', 'eye', 'nose', 'mouth', 'come', 'close', 'saw', 'clearly', 'humpty', 'dumpty', 'cant', 'anybody', 'else', 'say', 'im', 'certain', 'name', 'write', 'face', 'might', 'write', 'hundred', 'time', 'easily', 'enormous', 'face', 'humpty', 'dumpty', 'sit', 'leg', 'cross', 'like', 'turk', 'top', 'high', 'wallsuch', 'narrow', 'one', 'alice', 'quite', 'wonder', 'could', 'keep', 'balanceand', 'eye', 'steadily', 'fix', 'opposite', 'direction', 'didnt', 'take', 'least', 'notice', 'think', 'must', 'stuff', 'figure', 'exactly', 'like', 'egg', 'say', 'aloud', 'stand', 'hand', 'ready', 'catch', 'every', 'moment', 'expect', 'fall', 'provoke', 'humpty', 'dumpty', 'say', 'long', 'silence', 'look', 'away', 'alice', 'speak', 'call', 'eggvery', 'say', 'look', 'like', 'egg', 'sir', 'alice', 'gently', 'explain', 'egg', 'pretty', 'know', 'add', 'hop', 'turn', 'remark', 'sort', 'compliment', 'people', 'say', 'humpty', 'dumpty', 'look', 'away', 'usual', 'sense', 'baby', 'alice', 'didnt', 'know', 'say', 'wasnt', 'like', 'conversation', 'think', 'never', 'say', 'anything', 'fact', 'last', 'remark', 'evidently', 'address', 'treeso', 'stand', 'softly', 'repeat', 'humpty', 'dumpty', 'sit', 'wall', 'humpty', 'dumpty', 'great', 'fall', 'king', 'horse', 'king', 'men', 'couldnt', 'put', 'humpty', 'dumpty', 'place', 'last', 'line', 'much', 'long', 'poetry', 'add', 'almost', 'loud', 'forget', 'humpty', 'dumpty', 'would', 'hear', 'dont', 'stand', 'chatter', 'like', 'humpty', 'dumpty', 'say', 'look', 'first', 'time', 'tell', 'name', 'business', 'name', 'alice', 'stupid', 'enough', 'name', 'humpty', 'dumpty', 'interrupt', 'impatiently', 'mean', 'must', 'name', 'mean', 'something', 'alice', 'ask', 'doubtfully', 'course', 'must', 'humpty', 'dumpty', 'say', 'short', 'laugh', 'name', 'mean', 'shape', 'amand', 'good', 'handsome', 'shape', 'name', 'like', 'might', 'shape', 'almost', 'sit', 'alone', 'say', 'alice', 'wish', 'begin', 'argument', 'there', 'nobody', 'cry', 'humpty', 'dumpty', 'think', 'didnt', 'know', 'answer', 'ask', 'another', 'dont', 'think', 'youd', 'safe', 'grind', 'alice', 'go', 'idea', 'make', 'another', 'riddle', 'simply', 'good', 'natured', 'anxiety', 'queer', 'creature', 'wall', 'narrow', 'tremendously', 'easy', 'riddle', 'ask', 'humpty', 'dumpty', 'growl', 'course', 'dont', 'think', 'ever', 'fall', 'offwhich', 'there', 'chance', 'ofbut', 'purse', 'lip', 'look', 'solemn', 'grand', 'alice', 'could', 'hardly', 'help', 'laugh', 'fall', 'go', 'king', 'promise', 'mewith', 'mouthtoto', 'send', 'horse', 'men', 'alice', 'interrupt', 'rather', 'unwisely', 'declare', 'thats', 'bad', 'humpty', 'dumpty', 'cry', 'break', 'sudden', 'passion', 'youve', 'listen', 'doorsand', 'behind', 'treesand', 'chimneysor', 'couldnt', 'know', 'havent', 'indeed', 'alice', 'say', 'gently', 'book', 'ah', 'well', 'may', 'write', 'thing', 'book', 'humpty', 'dumpty', 'say', 'calm', 'tone', 'thats', 'call', 'history', 'england', 'take', 'good', 'look', 'im', 'one', 'speak', 'king', 'mayhap', 'youll', 'never', 'see', 'another', 'show', 'im', 'proud', 'may', 'shake', 'hand', 'grin', 'almost', 'ear', 'ear', 'lean', 'forward', 'nearly', 'possible', 'fell', 'wall', 'offer', 'alice', 'hand', 'watch', 'little', 'anxiously', 'take', 'smile', 'much', 'end', 'mouth', 'might', 'meet', 'behind', 'think', 'dont', 'know', 'would', 'happen', 'head', 'im', 'afraid', 'would', 'come', 'yes', 'horse', 'men', 'humpty', 'dumpty', 'go', 'theyd', 'pick', 'minute', 'would', 'however', 'conversation', 'go', 'little', 'fast', 'let', 'go', 'back', 'last', 'remark', 'one', 'im', 'afraid', 'cant', 'quite', 'remember', 'alice', 'say', 'politely', 'case', 'start', 'fresh', 'say', 'humpty', 'dumpty', 'turn', 'choose', 'subject', 'talk', 'game', 'think', 'alice', 'here', 'question', 'old', 'say', 'alice', 'make', 'short', 'calculation', 'say', 'seven', 'year', 'six', 'month', 'wrong', 'humpty', 'dumpty', 'exclaim', 'triumphantly', 'never', 'say', 'word', 'like', 'though', 'mean', 'old', 'alice', 'explain', 'id', 'mean', 'id', 'say', 'say', 'humpty', 'dumpty']\n"
     ]
    }
   ],
   "source": [
    "# importing regex and nltk\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# importing Counter to get word counts for bag of words\n",
    "from collections import Counter\n",
    "# importing a passage from Through the Looking Glass\n",
    "from looking_glass import looking_glass_text\n",
    "# importing part-of-speech function for lemmatization\n",
    "from part_of_speech import get_part_of_speech\n",
    "\n",
    "# Change text to another string:\n",
    "text = looking_glass_text\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "# Comment out the print statement below\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " however the egg only got larger and larger and more and more human when she had come within a few yards of it she saw that it had eyes and a nose and mouth and when she had come close to it she saw clearly that it was humpty dumpty himself it cant be anybody else she said to herself im as certain of it as if his name were written all over his face it might have been written a hundred times easily on that enormous face humpty dumpty was sitting with his legs crossed like a turk on the top of a high wallsuch a narrow one that alice quite wondered how he could keep his balanceand as his eyes were steadily fixed in the opposite direction and he didnt take the least notice of her she thought he must be a stuffed figure after all and how exactly like an egg he is she said aloud standing with her hands ready to catch him for she was every moment expecting him to fall its very provoking humpty dumpty said after a long silence looking away from alice as he spoke to be called an eggvery i said you looked like an egg sir alice gently explained and some eggs are very pretty you know she added hoping to turn her remark into a sort of a compliment some people said humpty dumpty looking away from her as usual have no more sense than a baby alice didnt know what to say to this it wasnt at all like conversation she thought as he never said anything to her in fact his last remark was evidently addressed to a treeso she stood and softly repeated to herself humpty dumpty sat on a wall humpty dumpty had a great fall all the kings horses and all the kings men couldnt put humpty dumpty in his place again that last line is much too long for the poetry she added almost out loud forgetting that humpty dumpty would hear her dont stand there chattering to yourself like that humpty dumpty said looking at her for the first time but tell me your name and your business my name is alice but its a stupid enough name humpty dumpty interrupted impatiently what does it mean must a name mean something alice asked doubtfully of course it must humpty dumpty said with a short laugh my name means the shape i amand a good handsome shape it is too with a name like yours you might be any shape almost why do you sit out here all alone said alice not wishing to begin an argument why because theres nobody with me cried humpty dumpty did you think i didnt know the answer to that ask another dont you think youd be safer down on the ground alice went on not with any idea of making another riddle but simply in her good natured anxiety for the queer creature that wall is so very narrow what tremendously easy riddles you ask humpty dumpty growled out of course i dont think so why if ever i did fall offwhich theres no chance ofbut if i did here he pursed his lips and looked so solemn and grand that alice could hardly help laughing if i did fall he went on the king has promised mewith his very own mouthtoto to send all his horses and all his men alice interrupted rather unwisely now i declare thats too bad humpty dumpty cried breaking into a sudden passion youve been listening at doorsand behind treesand down chimneysor you couldnt have known it i havent indeed alice said very gently its in a book ah well they may write such things in a book humpty dumpty said in a calmer tone thats what you call a history of england that is now take a good look at me im one that has spoken to a king i am mayhap youll never see such another and to show you im not proud you may shake hands with me and he grinned almost from ear to ear as he leant forwards and as nearly as possible fell off the wall in doing so and offered alice his hand she watched him a little anxiously as she took it if he smiled much more the ends of his mouth might meet behind she thought and then i dont know what would happen to his head im afraid it would come off yes all his horses and all his men humpty dumpty went on theyd pick me up again in a minute they would however this conversation is going on a little too fast lets go back to the last remark but one im afraid i cant quite remember it alice said very politely in that case we start fresh said humpty dumpty and its my turn to choose a subject he talks about it just as if it was a game thought alice so heres a question for you how old did you say you were alice made a short calculation and said seven years and six months wrong humpty dumpty exclaimed triumphantly you never said a word like it i though you meant how old are you alice explained if id meant that id have said it said humpty dumpty \n"
     ]
    }
   ],
   "source": [
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'humpty': 19, 'dumpty': 19, 'say': 19, 'alice': 16, 'name': 7, 'like': 7, 'think': 7, 'look': 6, 'im': 5, 'know': 5, 'mean': 5, 'go': 5, 'egg': 4, 'fall': 4, 'king': 4, 'would': 4, 'dont': 4, 'come': 3, 'write': 3, 'might': 3, 'sit': 3, 'one': 3, 'didnt': 3, 'take': 3, 'must': 3, 'stand': 3, 'hand': 3, 'remark': 3, 'never': 3, 'last': 3, 'wall': 3, 'horse': 3, 'men': 3, 'almost': 3, 'ask': 3, 'shape': 3, 'good': 3, 'another': 3, 'however': 2, 'large': 2, 'saw': 2, 'eye': 2, 'mouth': 2, 'cant': 2, 'face': 2, 'time': 2, 'narrow': 2, 'quite': 2, 'could': 2, 'long': 2, 'away': 2, 'speak': 2, 'call': 2, 'gently': 2, 'explain': 2, 'add': 2, 'turn': 2, 'conversation': 2, 'couldnt': 2, 'much': 2, 'interrupt': 2, 'course': 2, 'short': 2, 'laugh': 2, 'there': 2, 'cry': 2, 'make': 2, 'riddle': 2, 'thats': 2, 'behind': 2, 'book': 2, 'may': 2, 'ear': 2, 'little': 2, 'afraid': 2, 'old': 2, 'id': 2, 'get': 1, 'human': 1, 'within': 1, 'yard': 1, 'nose': 1, 'close': 1, 'clearly': 1, 'anybody': 1, 'else': 1, 'certain': 1, 'hundred': 1, 'easily': 1, 'enormous': 1, 'leg': 1, 'cross': 1, 'turk': 1, 'top': 1, 'high': 1, 'wallsuch': 1, 'wonder': 1, 'keep': 1, 'balanceand': 1, 'steadily': 1, 'fix': 1, 'opposite': 1, 'direction': 1, 'least': 1, 'notice': 1, 'stuff': 1, 'figure': 1, 'exactly': 1, 'aloud': 1, 'ready': 1, 'catch': 1, 'every': 1, 'moment': 1, 'expect': 1, 'provoke': 1, 'silence': 1, 'eggvery': 1, 'sir': 1, 'pretty': 1, 'hop': 1, 'sort': 1, 'compliment': 1, 'people': 1, 'usual': 1, 'sense': 1, 'baby': 1, 'wasnt': 1, 'anything': 1, 'fact': 1, 'evidently': 1, 'address': 1, 'treeso': 1, 'softly': 1, 'repeat': 1, 'great': 1, 'put': 1, 'place': 1, 'line': 1, 'poetry': 1, 'loud': 1, 'forget': 1, 'hear': 1, 'chatter': 1, 'first': 1, 'tell': 1, 'business': 1, 'stupid': 1, 'enough': 1, 'impatiently': 1, 'something': 1, 'doubtfully': 1, 'amand': 1, 'handsome': 1, 'alone': 1, 'wish': 1, 'begin': 1, 'argument': 1, 'nobody': 1, 'answer': 1, 'youd': 1, 'safe': 1, 'grind': 1, 'idea': 1, 'simply': 1, 'natured': 1, 'anxiety': 1, 'queer': 1, 'creature': 1, 'tremendously': 1, 'easy': 1, 'growl': 1, 'ever': 1, 'offwhich': 1, 'chance': 1, 'ofbut': 1, 'purse': 1, 'lip': 1, 'solemn': 1, 'grand': 1, 'hardly': 1, 'help': 1, 'promise': 1, 'mewith': 1, 'mouthtoto': 1, 'send': 1, 'rather': 1, 'unwisely': 1, 'declare': 1, 'bad': 1, 'break': 1, 'sudden': 1, 'passion': 1, 'youve': 1, 'listen': 1, 'doorsand': 1, 'treesand': 1, 'chimneysor': 1, 'havent': 1, 'indeed': 1, 'ah': 1, 'well': 1, 'thing': 1, 'calm': 1, 'tone': 1, 'history': 1, 'england': 1, 'mayhap': 1, 'youll': 1, 'see': 1, 'show': 1, 'proud': 1, 'shake': 1, 'grin': 1, 'lean': 1, 'forward': 1, 'nearly': 1, 'possible': 1, 'fell': 1, 'offer': 1, 'watch': 1, 'anxiously': 1, 'smile': 1, 'end': 1, 'meet': 1, 'happen': 1, 'head': 1, 'yes': 1, 'theyd': 1, 'pick': 1, 'minute': 1, 'fast': 1, 'let': 1, 'back': 1, 'remember': 1, 'politely': 1, 'case': 1, 'start': 1, 'fresh': 1, 'choose': 1, 'subject': 1, 'talk': 1, 'game': 1, 'here': 1, 'question': 1, 'calculation': 1, 'seven': 1, 'year': 1, 'six': 1, 'month': 1, 'wrong': 1, 'exclaim': 1, 'triumphantly': 1, 'word': 1, 'though': 1})\n"
     ]
    }
   ],
   "source": [
    "# Define bag_of_looking_glass_words & print:\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Models: N-Gram and NLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('the',), 1575), (('and',), 850), (('a',), 758), (('to',), 728), (('it',), 557), (('she',), 524), (('i',), 509), (('you',), 501), (('of',), 484), (('said',), 470)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the',), 1575), (('and',), 850), (('a',), 758), (('to',), 728), (('it',), 557), (('she',), 524), (('i',), 509), (('you',), 501), (('of',), 484), (('said',), 470)]\n",
      "\n",
      "Looking Glass n-grams:\n",
      "[(('the',), 1575), (('and',), 850), (('a',), 758), (('to',), 728), (('it',), 557), (('she',), 524), (('i',), 509), (('you',), 501), (('of',), 484), (('said',), 470)]\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "# importing ngrams module from nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from looking_glass import looking_glass_full_text\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 1)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 1)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 1)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n",
      "\n",
      "Looking Glass n-grams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 3)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n",
      "\n",
      "Looking Glass n-grams:\n",
      "[(('one', 'and', 'one', 'and', 'one'), 8), (('and', 'one', 'and', 'one', 'and'), 7), (('for', 'a', 'minute', 'or', 'two'), 6), (('the', 'lion', 'and', 'the', 'unicorn'), 6), (('as', 'well', 'as', 'she', 'could'), 5), (('is', 'worth', 'a', 'thousand', 'pounds'), 4), (('the', 'walrus', 'and', 'the', 'carpenter'), 4), (('said', 'to', 'herself', 'as', 'she'), 4), (('twas', 'brillig', 'and', 'the', 'slithy'), 3), (('brillig', 'and', 'the', 'slithy', 'toves'), 3)]\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 5)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Topics found by bag of words LDA ~~~\n",
      "Topic #1: dismantled deeply earth holder\n",
      "Topic #2: said dear yet photograph\n",
      "Topic #3: said son case young\n",
      "Topic #4: said son word old\n",
      "Topic #5: said street hand photograph\n",
      "Topic #6: father case mr said\n",
      "Topic #7: said father case young\n",
      "Topic #8: said time back son\n",
      "Topic #9: said hand street photograph\n",
      "Topic #10: father nothing found case\n",
      "\n",
      "\n",
      "~~~ Topics found by tf-idf LDA ~~~\n",
      "Topic #1: quick first person according\n",
      "Topic #2: still produce minor part\n",
      "Topic #3: holmes way love certainly\n",
      "Topic #4: strong baby resolute afraid\n",
      "Topic #5: heard dead white probable\n",
      "Topic #6: visit seems quietly horse\n",
      "Topic #7: since inquest action saturday\n",
      "Topic #8: flight reasoning scotland impression\n",
      "Topic #9: holmes man said upon\n",
      "Topic #10: passage appointment lady brown\n"
     ]
    }
   ],
   "source": [
    "from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3\n",
    "#from preprocessing import preprocess_text\n",
    "from text_preprocessing import preprocess_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# preparing the text\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]\n",
    "\n",
    "# Update stop_list:\n",
    "stop_list = [\"say\", \"see\", \"holmes\", \"shall\", \"say\", \n",
    "\"man\", \"upon\", \"know\", \"quite\", \"one\", \n",
    "\"well\", \"could\", \"would\", \"take\", \"may\", \n",
    "\"think\", \"come\", \"go\", \"little\", \"must\", \n",
    "\"look\"]\n",
    "# filtering topics for stop words\n",
    "def filter_out_stop_words(corpus):\n",
    "  no_stops_corpus = []\n",
    "  for chapter in corpus:\n",
    "    no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n",
    "    no_stops_corpus.append(no_stops_chapter)\n",
    "  return no_stops_corpus\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words model\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "# creating the tf-idf model\n",
    "tfidf_creator = TfidfVectorizer(min_df = 0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words LDA model\n",
    "lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n",
    "\n",
    "# creating the tf-idf LDA model\n",
    "lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
